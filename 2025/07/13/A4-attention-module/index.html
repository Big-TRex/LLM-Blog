<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/LLM-Blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/LLM-Blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/LLM-Blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/LLM-Blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/LLM-Blog/css/main.css">


<link rel="stylesheet" href="/LLM-Blog/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"big-trex.github.io","root":"/LLM-Blog/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="对于本次作业，我们将继续 Modeling 任务，以帮助你更深入地理解 Transformer 的各个组成模块。本次将特别关注 Transformer 结构核心的关键层之一：Attention Layer（注意力层）。 Task 1: Offline Sliding-Window AttentionMulti-head Attention（多头注意力机制） 模块是 Transformer 中一个至">
<meta property="og:type" content="article">
<meta property="og:title" content="A4 Attention Module">
<meta property="og:url" content="https://big-trex.github.io/2025/07/13/A4-attention-module/index.html">
<meta property="og:site_name" content="LLM-Assignment-Doc">
<meta property="og:description" content="对于本次作业，我们将继续 Modeling 任务，以帮助你更深入地理解 Transformer 的各个组成模块。本次将特别关注 Transformer 结构核心的关键层之一：Attention Layer（注意力层）。 Task 1: Offline Sliding-Window AttentionMulti-head Attention（多头注意力机制） 模块是 Transformer 中一个至">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://big-trex.github.io/2025/07/13/A4-attention-module/mask.svg">
<meta property="og:image" content="https://big-trex.github.io/2025/07/13/A4-attention-module/window.svg">
<meta property="og:image" content="https://big-trex.github.io/2025/07/13/A4-attention-module/bottom-right.svg">
<meta property="article:published_time" content="2025-07-13T03:01:59.000Z">
<meta property="article:modified_time" content="2025-07-15T06:28:20.648Z">
<meta property="article:author" content="DeepEngine">
<meta property="article:tag" content="Attention">
<meta property="article:tag" content="Mask">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://big-trex.github.io/2025/07/13/A4-attention-module/mask.svg">

<link rel="canonical" href="https://big-trex.github.io/2025/07/13/A4-attention-module/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>A4 Attention Module | LLM-Assignment-Doc</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/LLM-Blog/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">LLM-Assignment-Doc</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/LLM-Blog/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/LLM-Blog/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/LLM-Blog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://big-trex.github.io/2025/07/13/A4-attention-module/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/LLM-Blog/images/avatar.gif">
      <meta itemprop="name" content="DeepEngine">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LLM-Assignment-Doc">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          A4 Attention Module
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-07-13 11:01:59" itemprop="dateCreated datePublished" datetime="2025-07-13T11:01:59+08:00">2025-07-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-07-15 14:28:20" itemprop="dateModified" datetime="2025-07-15T14:28:20+08:00">2025-07-15</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/LLM-Blog/categories/Assignment/" itemprop="url" rel="index"><span itemprop="name">Assignment</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>对于本次作业，我们将继续 Modeling 任务，以帮助你更深入地理解 Transformer 的各个组成模块。本次将特别关注 Transformer 结构核心的关键层之一：<strong>Attention Layer（注意力层）</strong>。</p>
<h1 id="Task-1-Offline-Sliding-Window-Attention"><a href="#Task-1-Offline-Sliding-Window-Attention" class="headerlink" title="Task 1: Offline Sliding-Window Attention"></a>Task 1: Offline Sliding-Window Attention</h1><p><strong>Multi-head Attention（多头注意力机制）</strong> 模块是 Transformer 中一个至关重要的构建单元（具体内容可见参考文献），接收三个张量作为输入：</p>
<ul>
<li>Query tensor（查询张量），记作 $\mathbf{Q}$，满足 $\mathbf{Q} \in \mathbb{R}^{\text{batch_size} \times \text{seq_len_q} \times \text{num_head_q} \times \text{head_dim}}$，记为 <code>[b, sq, hq, hd]</code>。</li>
<li>Key tensor（键张量） 和 Value tensor（值张量），记为$\mathbf{K}, \mathbf{V}$，两者具有相同的形状，满足 $\mathbf{K}, \mathbf{V} \in \mathbb{R}^{\text{batch_size} \times \text{seq_len_kv} \times \text{num_head_kv} \times \text{head_dim}}$，记为 <code>[b, skv, hkv, hd]</code>。</li>
</ul>
<p>$\mathbf{Q}, \mathbf{K}, \mathbf{V}$ 共同构成了 Attention 模块的输入，用于后续的注意力计算。注意，对于 Multi-head Attention，batch_size 和 num_head 都应该被看作是 <strong>batch-like</strong> 维，而对于 <code>seq_len</code> 维度，$\mathbf{Q}$ 中的每一张量 $\mathbf{q}_\text{i}$ 可以被看作是第 i 个 <code>token</code> 的潜在嵌入查询信息（embedded latent query message），用于从知识 $\mathbf{V}$ 中查询相关信息，而 $\mathbf{V}$ 中的每一张量 $\mathbf{v}_\text{j}$ 可以被看作是第 j 个 <code>token</code> 的潜在嵌入知识表示（embedded latent knowledge archive）。为了聚合 $\mathbf{V}$ 中所有重要的信息并忽略其他无关的信息，每个 $\mathbf{v}_\text{j}$ 都对应一个潜在嵌入关键词张量 $\mathbf{k}_\text{j}$。通过计算 $\mathbf{q}_\text{i}$ 和 $\mathbf{k}_\text{j}$ 的点积标量 $\mathbf{q}_\text{i}\mathbf{k}_\text{j}^\top$，我们可以得到关于 $\mathbf{q}_\text{i}$ 和 $\mathbf{v}_\text{j}$ 之间的<strong>相似度得分（similarity score）</strong>。最终，每个 $\mathbf{q}_\text{i}$ 对应的聚合结果 $\mathbf{o}_\text{i}$ 表示为对 $\mathbf{V}$ 中所有 $\mathbf{v}_\text{j}$ 的加权和，即 $\mathbf{o}_\text{i} := \sum\limits_j \mathbf{a}^{(i)}_j\mathbf{v}_\text{j}$，其中，权重向量 $\mathbf{a}^{(i)}$ 由上述每个查询 $\mathbf{q}_\text{i}$ 与所有关键词 $\mathbf{k}_\text{j}$ 的 <strong>归一化点积相似度（normalized dot-product similarity）</strong> 构成。至于权重的归一化方式，最常见的做法是应用 <code>softmax</code> 操作，这被称为一种“软最大化（soft maximalization）”操作。其目的是让模型只 <strong>关注（pay attention to）</strong> 那些真正重要、即相似度得分最高的信息。</p>
<p>因此，整个 Attention 操作（针对每个 <code>batch</code> 和每个 <code>head</code>）可以表示为：</p>
<script type="math/tex; mode=display">
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \mathbf{A} \times \mathbf{V}</script><script type="math/tex; mode=display">
\text{where} \space \mathbf{A} = \text{softmax}_{\text{row-wise}}(\text{scale} · \mathbf{P}) \in \mathbb{R}^{\text{sq} \times \text{skv}}</script><script type="math/tex; mode=display">
\space \mathbf{P} = \mathbf{Q} \times \mathbf{K}^\top + \mathbf{M}\in \mathbb{R}^{\text{sq} \times \text{skv}}</script><p>其中，上式中 $\mathbf{M}$ 用于实现注意力机制中的<strong>掩码机制</strong>，$\mathbf{M}$ 是一个二值 <code>Attention Mask</code>，其每个元素的取值为 $-\infty$ 或 $0$，用于在计算注意力时进行筛选：</p>
<ul>
<li>若某一对 ($\mathbf{q}_\text{i}$, $\mathbf{k}_\text{j}$) 是无关的，则对应位置的 <code>Mask</code> 值为 $−\infty$，从而在 <code>softmax</code> 中被强制置接近 $0$（即被“屏蔽”）；</li>
<li>若该对是相关的，则对应位置的 <code>Mask</code> 值为 $0$，保留其注意力得分。</li>
</ul>
<p>常见的掩码模式包括：</p>
<ul>
<li>全量掩码（Full Mask）：每个 <code>token</code> 可以关注所有 <code>token</code>，如图 a；</li>
<li>因果掩码（Causal Mask）：每个 <code>token</code> 只能关注它之前的 <code>token</code> 及其自身。即 $\mathbf{q}_\text{i}$ 只能最多关注 $\mathbf{k}_\text{j}$ 满足 $\text{j} \leq \text{i}$ 的情况，不能看到未来的信息，如图 b；</li>
<li>滑动窗口掩码（Sliding Window Mask）：每个 <code>token</code> 只能关注窗口内的 <code>token</code>：<ul>
<li>对于 <code>Full</code> 场景， $\mathbf{q}_\text{i}$ 只能最多关注 $\mathbf{k}_\text{j}$ 满足 $\text{j} \in [i-w, i+w]$，如图 c；</li>
<li>对于 <code>Causal</code> 场景， $\mathbf{q}_\text{i}$ 只能最多关注 $\mathbf{k}_\text{j}$ 满足 $\text{j} \in [i-w, i]$，如图 d。</li>
</ul>
</li>
</ul>
<p><img src="mask.svg" alt="mask"></p>
<p>另外，由于 <code>softmax</code> 操作对数值变化非常敏感，一般会采取一些策略来稳定其计算过程。最常见的做法是对 <code>softmax</code> 的输入 $\mathbf{P}$ 进行缩放处理，即 $\text{scale} · \mathbf{P}$，其中，<code>scale</code> 通常设置为 $\frac{1}{\sqrt{hd}}$，以防止当维度增大时，数值激增而导致梯度不稳定。最近，Nvidia 在其论文中还引入了一些额外的技巧，用于在训练过程中进一步提升 <code>softmax</code> 操作的稳定性（详见参考文献中的 Nvidia 论文），我们也将采用其中的一些方法，具体包括：</p>
<ol>
<li><strong>Softmax Temperature（温度系数）</strong>：为了控制 <code>softmax</code> 分布的尖锐程度（sharpness），我们可以对 <code>softmax</code> 输入 $\mathbf{P}$ 应用温度系数，形式为：$\frac{\mathbf{P}}{temp}$。其中，<code>temp</code> 是一个取值范围在 $(0, +\infty)$ 的超参数，通过调节 temperature，可以控制模型对高相似度的响应程度，是调节注意力权重敏感度的重要手段：<ul>
<li>当 $\text{temp} = 1.0$ 时，<code>softmax</code> 分布是原始分布；</li>
<li>当 $\text{temp} \to 0.0$ 时，<code>softmax</code> 分布变得更加尖锐（sharp），即更接近 <code>one-hot</code>；</li>
<li>当 $\text{temp} \to +\infty$ 时，<code>softmax</code> 分布则变得更加平滑（smooth），各项概率更接近均匀分布。</li>
</ul>
</li>
<li><strong>Softmax Capping（上限截断）</strong>：除了使用 <code>softmax temperature</code> 外，我们还可以通过 <code>softmax capping</code> 来自适应地控制 $\mathbf{P}$ 的数值范围,形式为：$\text{cap} \cdot \text{tanh}(\frac{\mathbf{P}}{\text{cap}})$。其中，<code>cap</code> 通常是一个较大的正数，这种方法的作用类似于一个自适应版本的 <code>softmax temperature</code>：<ul>
<li>当 $\mathbf{P}$ 较小时，输出几乎不变；</li>
<li>当 $\mathbf{P}$ 较大时，使用 <code>tanh</code> 对其进行平滑限制，防止极端值导致梯度不稳定；</li>
<li>由于 <code>softmax capping</code> 和 <code>softmax temperature</code> 都是为了调控 <code>softmax</code> 的数值稳定性，因此在一次前向传播中，我们只使用其中一个。</li>
</ul>
</li>
<li><strong>Softmax Clipping（剪裁）</strong>：为了抑制 <code>Attention</code> 权重 $\mathbf{A}$ 中的异常值（outliers）过大增长，我们可以对 $\mathbf{A}$ 应用 <code>softmax clipping</code>，具体操作为：$\mathbf{A}_{\text{clipped}} = \text{clip} \left( (r - l) \cdot \mathbf{A} + l,\ 0,\ 1 \right)$，这种方法在不破坏归一化的前提下，有效地限制了 <code>Attention</code> 分布中的离群值，增强数值稳定性并降低过拟合风险，其中：<ul>
<li>$\mathbf{A}$ 是原始的 <code>softmax</code> 输出，数值范围在 $[0, 1]$；</li>
<li>$[l, r]$ 是一个扩展范围（super-range），满足 $l \leq 0，r \geq 1$；</li>
<li>这一步操作先将 $\mathbf{A}$ 从 $[0, 1]$ 线性映射到 $[l, r]$ 区间，再 <strong>clip（裁剪）</strong> 回 $[0, 1]$，从而截断极端值。</li>
</ul>
</li>
<li><strong>Softmax Dropout（注意力丢弃）</strong>：为了提升注意力权重 $\mathbf{A}$ 的鲁棒性（robustness），我们可以对 $\mathbf{A}$ 应用 <code>softmax dropout</code>，形式为：$\mathbf{A}_{\text{dropout}} = \text{dropout}_p(\mathbf{A})$，其中：<ul>
<li>$p \in [0, 1]$ 是 <code>dropout rate</code>（丢弃率）；</li>
<li>该操作会随机将 $\mathbf{A}$ 中的部分权重置为 0，并相应地对其余部分进行缩放，以保持总和不变。</li>
</ul>
</li>
<li><strong>QK 层归一化（QK Layer Normalization）</strong>：为了进一步缓解 $\mathbf{P}$ 中可能出现的过大数值问题（这可能导致注意力权重 $\mathbf{A}$ 退化为近似 <code>one-hot</code> 形式），我们可以选择对 $\mathbf{Q}$ 和 $\mathbf{K}$ 预先应用 <code>Layer Normalization</code>（层归一化）。在本次作业中，我们将传统的 <code>Layer Normalization</code> 替换为 <code>Group RMS Normalization</code>（组 RMS 归一化），以充分利用我们在 <strong>A2</strong> 中实现的 <code>GroupRMSNorm</code> 模块。</li>
</ol>
<p>最终，整个 <code>OfflineSlidingWindowAttn</code> 操作（针对每个 <code>batch</code> 和每个 <code>head</code>）可以表示为：</p>
<script type="math/tex; mode=display">
\text{OfflineSlidingWindowAttention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \mathbf{\widehat A} \times \mathbf{V}</script><script type="math/tex; mode=display">
\text{where} \space \mathbf{\widehat A} = \text{dropout}\space_p(\text{clip}((r-l) \mathbf{\tilde A} + l, 0, 1))</script><script type="math/tex; mode=display">
\space \mathbf{\tilde A} = \text{softmax}\space_{\text{row-wise}}(\mathbf{\tilde P})</script><script type="math/tex; mode=display">
\space \mathbf{\tilde P} = \begin{cases}
\cfrac{\text{scale} \cdot \mathbf{\tilde Q} \times \mathbf{\tilde K}^\top}{\text{temp}} + \mathbf{M}_{\text{sw}} \space(+ \mathbf{M}_{\text{causal}}), & \text{softmax temperature} \\
\text{cap}\cdot \text{tanh}(\cfrac{\text{scale} \cdot \mathbf{\tilde Q} \times \mathbf{\tilde K}^\top}{\text{cap}}) + \mathbf{M}_{\text{sw}} \space(+ \mathbf{M}_{\text{causal}}), & \text{softmax capping} \\
\end{cases}</script><script type="math/tex; mode=display">
\text{where}\space \mathbf{\tilde Q} = \text{GroupRMSNorm}(\mathbf{Q}), \space \mathbf{\tilde K} = \text{GroupRMSNorm}(\mathbf{K})</script><p>同时，为了使 <code>OfflineSlidingWindowAttn</code> 模块更灵活地适应不同格式的输入，我们在 <code>src/modeling/attention.py</code> 中定义了一个枚举类 <code>AttnQKVPackFormat</code>，用于定义 $\mathbf{Q}$、$\mathbf{K}$、$\mathbf{V}$ 输入张量的打包方式：</p>
<ul>
<li><strong>AttnQKVPackFormat.Q_K_V</strong>：最常见的格式，其中 $\mathbf{Q}$、$\mathbf{K}$、$\mathbf{V}$ 是三个独立的张量；</li>
<li><strong>AttnQKVPackFormat.Q_KV</strong>：在这种格式下，$\mathbf{K}$、$\mathbf{V}$ 沿着 <code>num_heads</code> 维度被打包在一起，构成一个张量，而 $\mathbf{Q}$ 仍然是单独的张量；</li>
<li><strong>AttnQKVPackFormat.QKV</strong>：此格式下，$\mathbf{Q}$、$\mathbf{K}$、$\mathbf{V}$ 都沿 <code>num_heads</code> 维度打包成一个张量，这种情况下，$\mathbf{Q}$ 和 $\mathbf{K}$、$\mathbf{V}$ 的其他维度（如序列长度、batch 大小等）必须相同，以保证解包后的结构正确。</li>
</ul>
<p>另外，我们还在 <code>src/modeling/attention.py</code> 中设计了另一个枚举类 <code>AttnQKVLayout</code>，用于定义 $\mathbf{Q}$、$\mathbf{K}$、$\mathbf{V}$ 张量的形状布局（shape layout），以支持不同的输入格式：</p>
<ul>
<li><strong>AttnQKVLayout.BSHD</strong>：最常见的布局形式，满足 $\mathbf{Q},\mathbf{K},\mathbf{V} \in \mathbb{R}^{\text{batch_size} \times \text{seq_len_kv} \times \text{num_head_kv} \times \text{head_dim}}$ ，记为 <code>bshd</code>；</li>
<li><strong>AttnQKVLayout.SBHD</strong>：更适用于<strong>分布式环境（distributed environment）</strong> 的布局，满足 $\mathbf{Q},\mathbf{K},\mathbf{V} \in \mathbb{R}^{\text{seq_len_kv} \times \text{batch_size} \times \text{num_head_kv} \times \text{head_dim}}$ ，记为 <code>sbhd</code>；</li>
<li><strong>AttnQKVLayout.THD</strong>：最通用的布局格式，也称为 <code>varlen layout</code>（变长布局），满足 $\mathbf{Q},\mathbf{K},\mathbf{V} \in \mathbb{R}^{\text{total_seq_len} \times \text{num_head_kv} \times \text{head_dim}}$，在这种布局中，不存在显式的 <code>batch</code> 维度，所有长度不一的序列会沿着 <code>sequence</code> 维度拼接在一起，在这种情况下需要额外提供两个辅助输入，用于标识每条序列在拼接后的张量中的位置：<ul>
<li><code>cu_seqlens_q</code>；</li>
<li><code>cu_seqlens_kv</code>；<br>这两个张量都是 <code>int32</code> 类型，形状为 <code>[batch_size + 1]</code>，其中每一段 <code>[[cu_seqlens&#125;[i], cu_seqlens[i+1]]</code> 表示第 i 个样本在 $\mathbf{Q}$ 或 $\mathbf{K}$、$\mathbf{V}$ 中的 起止区间（start-end），在 <code>varlen layout</code> 场景的掩码模式可参考下图。（更多示例请参考 Flash Attention 接口中的相关内容。）</li>
</ul>
</li>
</ul>
<p><img src="window.svg" alt="sliding window"></p>
<h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><p><strong>完成 <code>src/modeling/attention.py</code> 中的 <code>OfflineSlidingWindowAttn</code> 模块</strong>，实现上述注意力机制运算，具体细节包括：</p>
<ul>
<li>参数中的 <code>dtype</code> 和 <code>device</code> 是用于 <code>GroupRMSNorm</code> 中可学习参数的，它们可能与 $\mathbf{Q}$、$\mathbf{K}$、$\mathbf{V}$ 的 <code>dtype</code> 和 <code>device</code> 不同。</li>
<li>返回的输出张量 $\mathbf{O}$ 的元属性（meta attributes），包括 <code>dtype</code>、<code>device</code> 和 <code>layout</code>，应当与 $\mathbf{Q}$ 保持一致。</li>
<li>只有当参数 <code>softmax_cap</code> 被设置为 <code>None</code> 时，才可以使用 <code>softmax_temp</code> 参数启用 <code>softmax</code> 温度策略（softmax temperature strategy）。</li>
<li>所有参数都会被保证处于其合法范围内。</li>
<li>$\mathbf{Q}$ 和 $\mathbf{K}$ 的 <code>GroupRMSNorm</code> 是 <code>OfflineSlidingWindowAttn</code> 模块中的<strong>独立子层（individual sub-layers）</strong>，因为 <code>GroupRMSNorm</code> 只接受形状为 <code>[batch_size, seq_len, hidden_size]</code> 的三维张量，而其中的 <code>hidden_size = num_heads * head_dim</code>，在 $\mathbf{Q}$ 和 $\mathbf{K}$ 之间可能不同。此外，我们确保 <code>head_dim</code> 可以被 <code>group_size</code> 整除，即不会存在某个 <code>group</code> 在 <code>hidden</code> 维度上跨越两个不同 <code>head</code> 的情况。</li>
<li>当 <code>num_heads</code> 在 $\mathbf{Q}$ 和 $\mathbf{K}$、$\mathbf{V}$ 之间不相同时（即 <code>MQA</code> 或 <code>GQA</code> 风格，满足 <code>num_q_head != num_kv_head</code> 且 <code>num_q_head % num_kv_head == 0</code>，详见参考文献中的相关论文），我们采用相同的 <code>kv-heads</code> 重复策略（kv-heads repeating strategy）来使 $\mathbf{Q}$ 与 $\mathbf{K}$、$\mathbf{V}$ 在 <code>head</code> 数上保持一致。（参见参考文献中的 Llama Attention Layer 和 PyTorch 的 <strong>repeat_interleave</strong> 函数了解更多细节）。</li>
<li>当 $\mathbf{Q}$ 和 $\mathbf{K}$、$\mathbf{V}$ 在 <code>sequence</code> 维度上不一致时（例如在 <code>cross-attention</code> 或 <code>autoregressive decoding</code> 阶段），<code>attention mask M</code> 就不是一个方阵，而是形状为 <code>[sq, skv]</code> 的长方形矩阵，也可以看作是一个从完整 <code>Attention</code> 方阵 <code>[max(sq, skv), max(sq, skv)]</code> 中“滑动”出的窗口（slide）。<strong>注意</strong>：此时对于 <code>Causal</code> 掩码场景，我们应该从这个完整的 <code>Attention</code> 方阵中选取哪一块长方形窗口？对于 <code>OfflineSlidingWindowAttn</code> 模块，我们选择对齐完整方阵右下角区域（即 <strong>bottom-right</strong> 的掩码模式），遵循 flash-attention 的设置，参考下图，。（详见参考文献中的 Flash Attention 接口了解更多示例。）</li>
</ul>
<p><img src="bottom-right.svg" style="width: 30%; height: auto;"></p>
<h2 id="Offline-Sliding-Window-Attention-小结"><a href="#Offline-Sliding-Window-Attention-小结" class="headerlink" title="Offline Sliding-Window Attention 小结"></a>Offline Sliding-Window Attention 小结</h2><p>总结来说，你需要实现 <code>OfflineSlidingWindowAttn</code> 模块。该模块接收以不同打包格式（<code>packing formats</code>）和不同布局（<code>layouts</code>）表示的 $\mathbf{Q}$、$\mathbf{K}$、$\mathbf{V}$ 作为输入（如果布局为 <code>AttnQKVLayout.THD</code>，则需额外提供 <code>cu_seqlens_q</code> 和 <code>cu_seqlens_kv</code>），执行上述所描述的 <code>offline sliding window attention</code> 操作，并返回一个与 $\mathbf{Q}$ 使用相同布局的输出张量 $\mathbf{O}$。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/LLM-Blog/tags/Attention/" rel="tag"># Attention</a>
              <a href="/LLM-Blog/tags/Mask/" rel="tag"># Mask</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/LLM-Blog/2025/06/29/A3-modeling-mlp/" rel="prev" title="A3 Modeling MLP">
      <i class="fa fa-chevron-left"></i> A3 Modeling MLP
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Task-1-Offline-Sliding-Window-Attention"><span class="nav-number">1.</span> <span class="nav-text">Task 1: Offline Sliding-Window Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#TODO"><span class="nav-number">1.1.</span> <span class="nav-text">TODO</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Offline-Sliding-Window-Attention-%E5%B0%8F%E7%BB%93"><span class="nav-number">1.2.</span> <span class="nav-text">Offline Sliding-Window Attention 小结</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">DeepEngine</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/LLM-Blog/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/LLM-Blog/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DeepEngine</span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div> -->

        








      </div>
    </footer>
  </div>

  
  <script src="/LLM-Blog/lib/anime.min.js"></script>
  <script src="/LLM-Blog/lib/velocity/velocity.min.js"></script>
  <script src="/LLM-Blog/lib/velocity/velocity.ui.min.js"></script>

<script src="/LLM-Blog/js/utils.js"></script>

<script src="/LLM-Blog/js/motion.js"></script>


<script src="/LLM-Blog/js/schemes/pisces.js"></script>


<script src="/LLM-Blog/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
